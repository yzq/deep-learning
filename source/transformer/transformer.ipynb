{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to finish this function on your own\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        query: (batch_size, num_heads, seq_len_q, d_k)\n",
    "        key: (batch_size, num_heads, seq_len_k, d_k)\n",
    "        value: (batch_size, num_heads, seq_len_v, d_v)\n",
    "        mask: Optional mask to prevent attention to certain positions\n",
    "    \"\"\"\n",
    "    # get the size of d_k using the query or the key\n",
    "    \n",
    "    # calculate the attention score using the formula given. Be vary of the dimension of Q and K. And what you need to transpose to achieve the desired results.\n",
    "\n",
    "    #YOUR CODE HERE\n",
    "    d_k = query.shape[-1]\n",
    "\n",
    "    scores = torch.matmul(query, key.transpose(2, 3)) / math.sqrt(d_k)\n",
    "\n",
    "    # hint 1: batch_size and num_heads should not change\n",
    "    # hint 2: nXm @ mXn -> nXn, but you cannot do nXm @ nXm, the right dimension of the left matrix should match the left dimension of the right matrix. The easy way I visualize it is as, who face each other must be same\n",
    "\n",
    "    # add inf is a mask is given, This is used for the decoder layer. You can use help for this if you want to. I did!!\n",
    "    #YOUR CODE HERE\n",
    "    if mask is not None:\n",
    "        scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # get the attention weights by taking a softmax on the scores, again be wary of the dimensions. You do not want to take softmax of batch_size or num_heads. Only of the values. How can you do that?\n",
    "    #YOUR CODE HERE\n",
    "    attention_weights = softmax(scores, dim=-1)\n",
    "\n",
    "    # return the attention by multiplying the attention weights with the Value (V)\n",
    "    #YOUR CODE HERE\n",
    "    return torch.matmul(attention_weights, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    #Let me write the initializer just for this class, so you get an idea of how it needs to be done\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" #think why?\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Note: use integer division //\n",
    "\n",
    "        # Create the learnable projection matrices\n",
    "        self.W_q = nn.Linear(d_model, d_model) #think why we are doing from d_model -> d_model\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "        #YOUR IMPLEMENTATION HERE\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "        scores = torch.matmul(query, key.transpose(2, 3)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = softmax(scores, dim=-1)\n",
    "        return torch.matmul(attention_weights, value)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        #get batch_size and sequence length\n",
    "        #YOUR CODE HERE\n",
    "        batch_size = query.shape[0]\n",
    "        seq_len = query.shape[2]\n",
    "\n",
    "        # 1. Linear projections\n",
    "        #YOUR CODE HERE\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "\n",
    "        # 2. Split into heads\n",
    "        #YOUR CODE HERE\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 3. Apply attention\n",
    "        #YOUR CODE HERE\n",
    "        output = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # 4. Concatenate heads\n",
    "        #YOUR CODE HERE\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # 5. Final projection\n",
    "        #YOUR CODE HERE\n",
    "        return self.W_o(output)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
