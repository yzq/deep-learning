{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to finish this function on your own\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        query: (batch_size, num_heads, seq_len_q, d_k)\n",
    "        key: (batch_size, num_heads, seq_len_k, d_k)\n",
    "        value: (batch_size, num_heads, seq_len_v, d_v)\n",
    "        mask: Optional mask to prevent attention to certain positions\n",
    "    \"\"\"\n",
    "    # get the size of d_k using the query or the key\n",
    "    \n",
    "    # calculate the attention score using the formula given. Be vary of the dimension of Q and K. And what you need to transpose to achieve the desired results.\n",
    "\n",
    "    #YOUR CODE HERE\n",
    "    d_k = query.shape[-1]\n",
    "\n",
    "    scores = torch.matmul(query, key.transpose(2, 3)) / math.sqrt(d_k)\n",
    "\n",
    "    # hint 1: batch_size and num_heads should not change\n",
    "    # hint 2: nXm @ mXn -> nXn, but you cannot do nXm @ nXm, the right dimension of the left matrix should match the left dimension of the right matrix. The easy way I visualize it is as, who face each other must be same\n",
    "\n",
    "    # add inf is a mask is given, This is used for the decoder layer. You can use help for this if you want to. I did!!\n",
    "    #YOUR CODE HERE\n",
    "    if mask is not None:\n",
    "        scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # get the attention weights by taking a softmax on the scores, again be wary of the dimensions. You do not want to take softmax of batch_size or num_heads. Only of the values. How can you do that?\n",
    "    #YOUR CODE HERE\n",
    "    attention_weights = softmax(scores, dim=-1)\n",
    "\n",
    "    # return the attention by multiplying the attention weights with the Value (V)\n",
    "    #YOUR CODE HERE\n",
    "    return torch.matmul(attention_weights, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    #Let me write the initializer just for this class, so you get an idea of how it needs to be done\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" #think why?\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Note: use integer division //\n",
    "\n",
    "        # Create the learnable projection matrices\n",
    "        self.W_q = nn.Linear(d_model, d_model) #think why we are doing from d_model -> d_model\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "        #YOUR IMPLEMENTATION HERE\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "        scores = torch.matmul(query, key.transpose(2, 3)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = softmax(scores, dim=-1)\n",
    "        return torch.matmul(attention_weights, value)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        #get batch_size and sequence length\n",
    "        #YOUR CODE HERE\n",
    "        batch_size = query.shape[0]\n",
    "        seq_len = query.shape[2]\n",
    "\n",
    "        # 1. Linear projections\n",
    "        #YOUR CODE HERE\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "\n",
    "        # 2. Split into heads\n",
    "        #YOUR CODE HERE\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 3. Apply attention\n",
    "        #YOUR CODE HERE\n",
    "        output = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # 4. Concatenate heads\n",
    "        #YOUR CODE HERE\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # 5. Final projection\n",
    "        #YOUR CODE HERE\n",
    "        return self.W_o(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Network\n",
    "\n",
    "    Args:\n",
    "        d_model: input/output dimension\n",
    "        d_ff: hidden dimension\n",
    "        dropout: dropout rate (default=0.1)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        #create a sequential ff model as mentioned in section 3.3\n",
    "        #YOUR CODE HERE\n",
    "        self.model = nn.Sequential(nn.Linear(d_model, d_ff),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(dropout),\n",
    "                      nn.Linear(d_ff, d_model),\n",
    "                      nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        #YOUR CODE HERE\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of shape (max_seq_length, d_model)\n",
    "        #YOUR CODE HERE\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        # Create position vector\n",
    "        #YOUR CODE HERE\n",
    "        position = torch.arange(0, max_seq_length).unsqueeze(1)\n",
    "\n",
    "        # Create division term\n",
    "        #YOUR CODE HERE\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Compute positional encodings\n",
    "        #YOUR CODE HERE\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Register buffer\n",
    "        #YOUR CODE HERE\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Multi-head attention\n",
    "        #YOUR CODE HERE\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        # 2. Layer normalization\n",
    "        #YOUR CODE HERE\n",
    "        self.layer_norm1 = nn.LayerNorm\n",
    "\n",
    "        # 3. Feed forward\n",
    "        #YOUR CODE HERE\n",
    "\n",
    "        # 4. Another layer normalization\n",
    "        #YOUR CODE HERE\n",
    "\n",
    "        # 5. Dropout\n",
    "        #YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            mask: Optional mask for padding\n",
    "        Returns:\n",
    "            x: Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # 1. Multi-head attention with residual connection and layer norm\n",
    "        #YOUR CODE HERE\n",
    "\n",
    "        # 2. Feed forward with residual connection and layer norm\n",
    "        #YOUR CODE HERE\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
