<think>
如果用户表示文件中没有内容，这可能意味着文件创建了但内容没有正确保存或显示。我需要重新提供Markdown内容，确保用户可以将其正确保存到文件中。

我将重新提供完整的Markdown内容，并使用简洁明了的说明。
</think>

# 文件内容缺失问题

看起来文件可能创建了但内容没有正确保存或显示。请将以下完整内容复制到您的`source/交叉熵损失函数解析.md`文件中：

```markdown
# 交叉熵损失函数解析

## 原始代码

```python
def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y])
```

## 交叉熵损失函数的代码解析

这段代码虽然简短，但确实实现了交叉熵损失函数的核心功能。

### 实现原理

交叉熵损失函数的数学定义是：
\[ H(y, \hat{y}) = -\sum_{i} y_i \log(\hat{y}_i) \]

在多分类问题中：
- `y_hat` 是模型预测的概率分布，形状为 (batch_size, num_classes)
- `y` 是真实标签的索引，形状为 (batch_size,)

由于分类问题中 `y` 实际上是one-hot向量（只有真实类别对应的位置是1，其他位置都是0），所以数学公式可以简化为：
\[ H(y, \hat{y}) = -\log(\hat{y}_c) \]
其中 c 是真实类别的索引。

### 代码分析

- `range(len(y_hat))` 创建了一个从0到batch_size-1的序列，表示每个样本的索引
- `y_hat[range(len(y_hat)), y]` 通过高级索引，提取每个样本中对应其真实标签的预测概率值
- `- torch.log(...)` 对这些概率值取对数并取负

例如，如果：
```python
y_hat = [[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]
y = [0, 2]
```

则 `y_hat[range(len(y_hat)), y]` 会选择 `y_hat[0, 0]` 和 `y_hat[1, 2]`，也就是 `tensor([0.1, 0.5])`，然后计算 `-torch.log(tensor([0.1, 0.5]))`，得到这两个样本的交叉熵损失。

## 关于类别表示的说明

在实际应用中，`y` 并不是独热向量，而是**类别索引**。这是深度学习框架（如PyTorch）中常用的一种简化方式：

### 两种类别表示方法

1. **独热向量表示**：例如三分类问题中
   - 类别0：[1, 0, 0]
   - 类别1：[0, 1, 0]
   - 类别2：[0, 0, 1]

2. **类别索引表示**：直接用整数表示类别
   - 类别0：0
   - 类别1：1
   - 类别2：2

### 在代码中的应用

在上述例子中：
```python
y_hat = [[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]
y = [0, 2]
```

这里的`y`值分别是0和2，表示第一个样本的真实类别是第0类，第二个样本的真实类别是第2类。

这种表示方法比独热向量更节省内存，也便于直接索引。代码`y_hat[range(len(y_hat)), y]`正是利用这些索引值从预测概率中提取出对应真实类别的概率。

交叉熵公式中的独热向量数学表示被简化为直接选取对应类别的预测概率，这正是这个函数的精妙之处。
```

请确保将此内容完整复制到文件中。如果您需要我以其他方式提供帮助，请告诉我。
